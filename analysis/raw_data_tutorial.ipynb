{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Getting Started\n","\n"," Following the instructions for generating agent data (either [locally](https://github.com/KDL-umass/ToyboxAgents/wiki/Generate-Agent-Data-Locally) or [on a cluster](https://github.com/KDL-umass/ToyboxAgents/wiki/Generate-Agent-Data-on-a-Cluster-Using-Slurm)), you will end up with a directory of saved game states that looks something like this:\n","\n"," ```\n"," {output}\n"," - {AgentClass1}\n","   - {seed1}\n","     - {AgentClass100001.json}\n","     - {AgentClass100001.png}\n","     - {AgentClass100002.json}\n","     - {AgentClass100002.png}\n","     - ...\n","   - {seed2}\n","     - {AgentClass100001.json}\n","     - {AgentClass100001.png}\n","     - {AgentClass100002.json}\n","     - {AgentClass100002.png}\n","     - ...\n","   - ...\n"," - {AgentClass2}\n"," - ...\n"," ```\n","\n"," `output` here refers to the directory provided as the argument to the code that produced the data (run `pythom -m agents --help` in the root directory )\n","\n"," # Load some data\n"," The first thing we'd like to do is load in some data. Assuming a gzipped archive called `agents.tar.gz`, we can load the relevant data using the `load_data` function from the local `utils` module:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 124876/124876 [00:45<00:00, 2746.56it/s]\n"}],"source":["from utils import load_data\n","\n","agent1 = load_data('data/agent1.tgz', load_state=True)\n","# Warning: loading all four agents takes time! \n","# agent2 = load_data('data/agent2.tgz', 'breakout' load_state=True) \n","# agent3 = load_data('data/agent3.tgz', 'breakout' load_state=True) \n","# agent4 = load_data('data/agent4.tgz', 'breakout' load_state=True) \n",""]},{"cell_type":"markdown","metadata":{},"source":[" You only need to load the data up once. Cells in notebooks are stateful and they can be run out of order (although in this tutorial, we have written it to be run in order). For interactive data analysis, we'd like to only load the data into memory once.\n","\n"," If you'd like to make videos for debugging, you will also need to load in the images and can run the following code; note that you can do this in one pass by additionally supplying the `load_images` argument in the code in the previous cell.\n","\n"," This code will create videos for each trial of each agent. Videos are saved in the format `<agentclass>_<seed>.mp4`. You may find these videos helpful for debugging."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 124876/124876 [01:55<00:00, 1083.44it/s]\n100%|██████████| 2245/2245 [00:02<00:00, 892.09it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1093.98it/s]\n100%|██████████| 2304/2304 [00:01<00:00, 1237.00it/s]\n100%|██████████| 2214/2214 [00:01<00:00, 1241.26it/s]\n100%|██████████| 2245/2245 [00:01<00:00, 1193.76it/s]\n100%|██████████| 2266/2266 [00:01<00:00, 1135.93it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1116.22it/s]\n100%|██████████| 2372/2372 [00:02<00:00, 1176.64it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1077.89it/s]\n100%|██████████| 2245/2245 [00:01<00:00, 1245.19it/s]\n100%|██████████| 2331/2331 [00:02<00:00, 1152.82it/s]\n100%|██████████| 2290/2290 [00:02<00:00, 1027.22it/s]\n100%|██████████| 2245/2245 [00:01<00:00, 1137.84it/s]\n100%|██████████| 2337/2337 [00:01<00:00, 1205.55it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1146.08it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1152.40it/s]\n100%|██████████| 2349/2349 [00:02<00:00, 1149.20it/s]\n100%|██████████| 2311/2311 [00:01<00:00, 1217.95it/s]\n100%|██████████| 2279/2279 [00:01<00:00, 1149.54it/s]\n100%|██████████| 2284/2284 [00:01<00:00, 1162.19it/s]\n100%|██████████| 2283/2283 [00:01<00:00, 1189.15it/s]\n100%|██████████| 2287/2287 [00:01<00:00, 1154.71it/s]\n100%|██████████| 2276/2276 [00:01<00:00, 1189.43it/s]\n100%|██████████| 2330/2330 [00:01<00:00, 1205.60it/s]\n100%|██████████| 2285/2285 [00:01<00:00, 1252.93it/s]\n100%|██████████| 2320/2320 [00:01<00:00, 1268.47it/s]\n100%|██████████| 2311/2311 [00:01<00:00, 1219.36it/s]\n100%|██████████| 1253/1253 [00:01<00:00, 1172.36it/s]\n100%|██████████| 2245/2245 [00:01<00:00, 1144.17it/s]\n100%|██████████| 2245/2245 [00:01<00:00, 1262.25it/s]\n"}],"source":["from utils import make_videos\n","agent1_videos = load_data('data/agent1.tgz', load_images=True)\n","make_videos(agent1_videos['images'])\n","del agent1_videos\n",""]},{"cell_type":"markdown","metadata":{},"source":["\n","\n"," # Defining an outcome variable\n","\n"," Although Breakout appears to be a simple game, there are actually quite a few outcomes variables we could define. For example, an obvious or rudimentary one that would be suitable for an explanation system might be, \"Why did you miss the ball?\" In our framing of explanation, this query would be converted to: \"What would need to have been different for the agent to hit the ball?\" Of course, the agent would actually need to have missed the ball at some point for this to be meaningful.\n","\n"," A low-level query that ought to be valid for all well-performing agents might be \"Why didn't you take action _a_ at time _t_?\", since at some point, every agent needs to move.\n","\n"," A third possible query might be, \"Why didn't you target column _i_ of the board?\"\n","\n"," ## Why didn't you take action _a_ at time _t_?\n","\n"," This is the easiest counterfactual query to define. `load_data` returns a dictionary that may contain keys `images`, `states`, and `actions`. The values in these dictionaries are themselves dictionaries, where their keys are the random seeds and the values are the timestamped actions, images, or states. The time-stamped content is not guaranteed to be sorted, so we will need to handle that."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# First choose the seed; states are organized by the agent's name. \n","# agent1 is the `Target` agent.\n","from typing import List, Tuple\n","seed = list(agent1['states']['Target'].keys())[0]\n","states = agent1['states']['Target'][seed]\n","actions : List[Tuple[str, str]] = agent1['actions']['Target'][seed]\n","\n","# The state list is a list of tuples. The first first entry in the tuple is \n","# the filename, which contains the left-padded frame number.\n","# The first entry in the action tuple is a left-padded string denoting the \n","# frame number that the action is responding to.\n","states.sort(key=lambda t: t[0])\n","actions.sort(key=lambda t: t[0])\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Now get the agent and the action at time step 100.\n","# Note: Frames are 1-indexed.\n","state = states[99]\n","action = actions[99]\n","\n","# Make sure they match\n","assert action[0] in state[0]\n","\n","# Now we have our first outcome:\n","outcome1 = action[1]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" If we wanted to run experiments for this particular scenario, we'd load up the prior state\n"," (i.e., `state[1]`), mutate it, load it into the Toybox engine, and then observe the outcome.\n"," ## Why did you miss the ball?\n"," Each trial ends either when the agent times out, or misses the ball. Therefore,\n"," this query will only be valid on a per-trial basis, so we just need to check what's going\n"," on in the final state. We can do this easily in python by loading up Toybox state as\n"," Python, using the interventions API:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Loaded Toybox environments.\n"},{"data":{"text/plain":"{'score': 289,\n 'rand': {'state': [17737625973469600953, 13918069903993558631]},\n 'lives': 0,\n 'level': 1,\n 'intervention': None,\n 'paddle': <toybox.interventions.breakout.Paddle at 0x3ecc02630>,\n 'reset': False,\n 'ball_radius': 2.0,\n 'bricks': <toybox.interventions.breakout.BrickCollection at 0x11463ef28>,\n 'balls': <toybox.interventions.breakout.BallCollection at 0x11463e630>,\n 'paddle_speed': 4.0,\n 'paddle_width': 24.0,\n 'is_dead': True}"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import toybox.interventions.breakout as breakout\n","\n","# Grab the last state in the list:\n","last_state = states[-1]\n","\n","# But these are tuples, so let's redefine it to be the actual state json:\n","last_state = last_state[1]\n","\n","# Now let's load up the object\n","last_state = breakout.Breakout.decode(None, last_state, breakout.Breakout)\n","\n","# Now we can see what's defined on this object:\n","vars(last_state)\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# It turns out that, as soon as the agent misses the ball, it's removed from the state\n","# information. Given the way we've set up data collection, this means that we can just \n","# check whether there are zero current balls:\n","missed = len(last_state.balls) == 0\n","\n","# We might want to double check this, though, so let's ensure that in the previous frame,\n","# the ball's y position is *greater than* the paddle's y position (coordinates are \n","# relative to the top left corner of the frame)\n","penultimate_state = breakout.Breakout.decode(None, states[-2][1], breakout.Breakout)\n","ball = penultimate_state.balls[0]\n","paddle = penultimate_state.paddle\n","\n","# Now we want to see if the ball's y-position is lower than the paddle's y-position:\n","assert ball.position.y > paddle.position.y\n","outcome2 = missed\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" One of the challenges with this particular outcome is that it is not clear what the window\n"," of relevant action is. When using experiments to evaluate the counterfactual for the\n"," purposes of explanation, we have limited ourselves to the window of one time-step.\n"," This actually limits the relevant queries for amongst eligible trials where the agents\n"," missed the ball -- it will only produce an satisfactory explanation if e.g. it did not move,\n"," or moved in the wrong direction at time t-1. If, instead, the agent was very far away\n"," from the ball, no action it could have taken at t-1 would have changed the outcome.\n"," ## Why didn't you target column _i_?\n"," _This example includes a lengthy discussion of the challenges of more complex outcomes.\n"," The primary audience for this section is CRA, but would be useful for anyone looking to\n"," define more sophisticated outcome variables_.\n","\n"," This query is similar to the previous one, but substantially harder, since there are\n"," several possible interpretations of it. The first challenge we face is that even the\n"," act of measuring the outcome itself is temporally extended, describing a complex\n"," behavior. Suppose we had perfect mathematical model of the bounce mechanics in Breakout.\n"," Then, given that the ball is traveling downward at time $t$, we could compute where the agent would\n"," need to align the paddle in order to hit the first alive brick in the target column.\n"," The agent would need to move the paddle to this location before the ball crosses the\n"," x-axis.\n","\n"," Let the time at which the ball crosses the x-axis be $t'$. Let the current paddle location be\n"," $p_t$.\n"," Assume that there is only one paddle position position at $t'$ that will allow the ball to\n"," hit the targeted column, (i.e., only one $p_{t'}$).\n"," Then if $t' - t < |p_t - p_{t'}|$, it is impossible for the agent to move the paddle to the\n"," correct location in time. Since we want $t' - t \\geq |p_t -p_{t'}|$, let's choose to measure\n"," from time $t$ such that that $t \\leq t' - |p_t - p_{t'}|$.\n","\n"," First consider the case where $t = t' - |p_t - p_{t'}|$. Let $M$ denote a\n"," function that models the bounce mechanics of the environment from time $t$ onward\n"," (this means that $M$ is conditioned on the state of the game at $t$ and is not defined for\n"," inputs that are less than $t$).\n"," $M$ outputs the location of the ball at any time $t_i$, and can be used in conjunction\n"," with the paddle location to compute the appropriate action that moves the paddle toward\n"," $p_{t'}$:\n"," $$ a_{t_i} = \\begin{cases}\\text{left}, & M(t_i) - p_{t_i} < 0\\\\\\text{right}, & M(t_i) - p_{t_i} > 0\\\\\\text{noop}, &\\text{o/w}\\end{cases}$$\n"," This action is optimal at time $t_i$: because we have chosen the smallest $t$ the will allow\n"," us to hit the ball at the correct paddle location, if we do not take action $a_{t_i}$,\n"," the ball will not hit any bricks in the column.\n","\n"," We happen to know that $a_{t_i}$ will be the same for the entire period $[t, t']$\n"," (i.e., $\\forall t_i\\in [t, t'], t_j \\in [t, t'], a_{t_i} = a_{t_j}$) because we have\n"," specifically chosen $t$ so that there will not be enough time for the agent to move past $t$.\n"," Therefore, we can just call this single optimal action $a^*$.\n","\n"," Now we can define our outcome variable over the window $[t, t']$. If $a_{t_i}$ is the action the\n"," agent actually took for at time $t_i$ some trial, then we are interested in the case where\n"," $\\exists t_i\\in [t, t'], a_{t_i} \\not=a^*$, i.e., when the agent chose an action that would cause it\n"," to no longer be targeting the column of interest.\n","\n"," This definition of how to measure whether an agent is \"targeting\" the ball\n"," at a column is incredibly rigid: It has no allowance for any error on the part of the agent.\n"," There may be some source of random error in the flow of information from the environment, through the\n"," agent's action selection procedure, through the implementation of that action. This errant behavior may\n"," disappear upon re-running the agent, or it may only manifest for _this_ particular column or state\n"," configuration.  Furhtermore, this definition cannot capture when the agent has learned a model that\n"," differs from the true bounce mechanics.\n"," While we can say with certainty that the agent has not learned $M$, we cannot say that it has not learned\n"," how to target a column more generally.\n","\n"," Our definition brings up another problem with this particular outcome: it might be reasonable to believe\n"," that a human desiring an explanation\n"," would only ask this counterfactual in cases where they believe the agent has actually learned to target\n"," the ball. For example, if the human observer sees an agent that is struggling to hit the ball in the\n"," first place, they might not find a query about targeting a column useful. See the two agents\n"," below (you may need to re-execute the code below for the video to display properly)."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":"<video width=\"45%\" controls>\n  <source src=\"videos/Target.mp4\" type=\"video/mp4\" />\n</video>\n<video width=\"45%\" controls>\n  <source src=\"videos/StayAliveJitter.mp4\" type=\"video/mp4\" />\n</video>\n","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}],"source":"%%HTML\n<video width=\"45%\" controls>\n  <source src=\"videos/Target.mp4\" type=\"video/mp4\" />\n</video>\n<video width=\"45%\" controls>\n  <source src=\"videos/StayAliveJitter.mp4\" type=\"video/mp4\" />\n</video>"},{"cell_type":"markdown","metadata":{},"source":[" This question of whether \"targeting\" is an appropriate outcome relates to our intuition that\n"," when agents exhibit complex behavior, it is because there is an internal representation or decision\n"," that has a mapping to the externally observed behavior, and that\n"," that decision procedure can be accurately measured using our\n"," externally-defined function. We might feel comfortable saying that the agent on the left\n"," exhibits targeting behavior, but the minor adjustments the agent makes as the ball approaches\n"," the paddle would likely make our earlier definition of the behavior insufficient. In fact,\n"," because this agent is scripted, we know that it contains some internal estimation of where\n"," the ball will land. The agent \"believes\" that it is targeting a specific column. This data\n"," latent, although it could be recovered via log files.\n","\n"," We happen to know that the agent on the right does not use any variables that are not\n"," available in the recorded state data. We can inspect the code to see that it does not\n"," engage in any \"targeting,\" but a human observer might say that it is \"targeting\" the left-most\n"," column. Furthermore, it is possible to write an agent that uses similar planning to the agent\n"," in the video on the right, but \"gets stuck\" when targeting the columns on either end of the\n"," playing area.\n","\n"," One possible solution to this problem is to include some tolerance in our definition of what\n"," it means to be \"targeting\" a column. A more relaxed version of the definition of this outcome\n"," would not only address the aforementioned issues with what we mean by \"targeting,\" but it would\n"," address the fact that:\n","\n"," * Our assumption that there is exactly one $p_{t'}$ (i.e., x-position of the paddle when the\n"," ball crosses the x-axis) may not be true.\n"," * In practice, selecting $t$ to be exactly equal to $t' - |p_t - p_{t'}|$ may be prohibitively\n"," expensive, because finding the exact value of $t$ may be expensive.\n","\n","\n"," One way to add some slack is to discretize the paddle. We happen to know that the default\n"," configuration of the paddle contains five discrete segments. However, if we did not know this,\n"," we could guess from observing agent behavior that there are at least three segments -- a\n"," middle, left, and right. Let $b_{t_i}$ be the ball position at time $t_i$. Paddle\n"," x-position (i.e., $p_{t_i}$) is the center of the paddle, so we could easily\n"," define a approximation where if $|p_{t_i} - b_{t_i}| < c$, then we are in the middle\n"," of the paddle, and the sign of the difference determines left and right.\n","\n"," Note that through this solution, we have side-stepped the temporal natural of this outcome\n"," by re-defining it using an approximation that can be measured at a single point in time.\n"," This single point in time still needs to be calculated back from the inciting event (i.e.,\n"," hitting a brick in column $j$, rather than column $i$). Our re-framing transforms this query\n"," into something more like \"Why did you miss the ball?\" However, rather than reasoning about\n"," a _sequence_ of actions, we are now reasoning about a single action. Future work would\n"," situate our framing in terms of literature on tasks, options, etc.\n","\n"," We defer computing examples of aiming behavior, since they will either be labor-intensive or\n"," compute-intensive: without prior knowledge of meaningful queries, we would need to identify\n"," every relevant time point for this query, and then compute every relevant $(i, j)$ pair.\n","\n"," # Generating CSVs for inference\n"," We now show how we generate data for a single agent. This is the first pass approach; we\n"," will discuss more efficient ways to do this in other tutorials. The purpose of this\n"," demonstration is to establish the workflow from raw data, which is important during the\n"," exploratory phase."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Grab the config; we will need information in here.\n","from ctoybox import Toybox\n","from collections import namedtuple\n","import numpy as np\n","import math\n","\n","# Grab the first 10 as an example\n","bstates = [breakout.Breakout.decode(None, s[1], breakout.Breakout) for s in states[:10]]\n","\n","# We are going to want to use some helper functions from the interventions API.\n","# To avoid cleaning up TB and having too many indentations, let's provide a dummy TB instance.\n","query = breakout.BreakoutIntervention(namedtuple('tb', 'game_name')('breakout'), 'breakout')\n","bactions = [a for (_, a) in actions]\n","\n","# The agents used the default settings, so we can just grab it from a fresh instance. \n","with Toybox('breakout') as tb:\n","    config = tb.config_to_json()\n","    query.config = config \n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import csv\n","\n","initial_paddle_width = None\n","prev_state = None\n","\n","with open('dat.csv', 'w') as f:\n","\n","    datawriter = csv.writer(f, delimiter=',')\n","    # Write the header\n","    datawriter.writerow([\n","        'agent_name', \n","        'seed', \n","        't', \n","        'action', \n","        'missed_ball', \n","        'xpos_ball', \n","        'ypos_ball', \n","        'xpos_ball_prev',\n","        'ypos_ball_prev',\n","        'xpos_pad',\n","        'ypos_pad',\n","        'xpos_pad_prev',\n","        'ypos_pad_prev',\n","        'indicators',\n","        'is_far_left',\n","        'is_far_right',\n","        'score',\n","        'pad_width',\n","        'ball_speed',\n","        'ball_down',\n","        'xdist_ball_pad',\n","        'ydist_ball_pad',\n","        'l2_ball_pad',\n","        'num_bricks_left'])\n","\n","    for t, state in enumerate(bstates):\n","        query.game = state\n","        if t == 0:\n","            initial_paddle_width = state.paddle_width\n","        record = ['agent1', seed, t]\n","\n","        # Record the action at time t\n","        # action\n","        record.append(bactions[t])\n","\n","        # Record whether the agent missed the ball\n","        missed_ball = len(state.balls) == 0\n","        # missed_ball\n","        record.append(missed_ball)\n","        \n","        # Record the x and y coordinates of the ball and paddle at t and t-1\n","        # Store intermediate values that we use later.\n","        ball_pos = state.balls[0].position\n","        paddle_pos = state.paddle.position\n","\n","        # xpos_ball\n","        record.append(ball_pos.x if not missed_ball else None)\n","        # ypos_ball\n","        record.append(ball_pos.y if not missed_ball else None)\n","        # xpos_ball_prev\n","        record.append(prev_state.balls[0].position.x if prev_state else None)\n","        # ypos_ball_prev\n","        record.append(prev_state.balls[0].position.y if prev_state else None)\n","        # xpos_pad\n","        record.append(paddle_pos.x)\n","        # ypos_pad\n","        record.append(paddle_pos.y)\n","        # xpos_pad_prev\n","        record.append(prev_state.paddle.position.x if prev_state else None)\n","        # ypos_pad_prev\n","        record.append(prev_state.paddle.position.y if prev_state else None)\n","\n","        # The binary representation of this number indicates whether the \n","        # column at the ith bit is a channel\n","        indicators = [query.get_column(i) for i in range(query.num_columns())]\n","        # indicators\n","        record.append(sum(2**i for i, v in enumerate(indicators) if v))\n","\n","        # Record whether the paddle is on the far left or far right of the screen\n","        leftmost_brick = query.get_column(0)[0]\n","        rightmost_brick = query.get_column(query.num_columns() - 1)[0]\n","        \n","        record.append(paddle_pos.x <= (leftmost_brick.position.x - (leftmost_brick.size.x * 0.5)))\n","        record.append(paddle_pos.x >= (rightmost_brick.position.x + (rightmost_brick.size.x * 0.5)))\n","\n","        # Record the score\n","        record.append(state.score)\n","\n","        # Paddle width can be two sizes\n","        record.append('big' if state.paddle_width == initial_paddle_width else 'small')\n","\n","        # Ball speed can have one of two values\n","        record.append(None if missed_ball else \\\n","            'slow' if math.isclose(state.balls[0].position.y, \n","                                   config['ball_speed_slow'], \n","                                   rel_tol=0.1) \\\n","            else 'fast')\n","\n","        # Record whether the ball is travelling downward\n","        # The origin is the top left, so downward movement increases the value of y\n","        record.append(None if missed_ball else state.balls[0].velocity.y > 0)\n","\n","        # Different types of distances between balls and paddles\n","        record.append(abs(ball_pos.x - paddle_pos.x))\n","        record.append(abs(ball_pos.y - paddle_pos.y))\n","        record.append(math.sqrt((ball_pos.x - paddle_pos.x)**2 + (ball_pos.y - paddle_pos.y)**2))\n","\n","        # Total bricks left\n","        record.append(sum(int(b.alive) for b in state.bricks))\n","\n","        # Write the row and set the prev_state to be the current state\n","        datawriter.writerow(record)\n","        prev_state = state\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>agent_name</th>\n      <th>seed</th>\n      <th>t</th>\n      <th>action</th>\n      <th>missed_ball</th>\n      <th>xpos_ball</th>\n      <th>ypos_ball</th>\n      <th>xpos_ball_prev</th>\n      <th>ypos_ball_prev</th>\n      <th>xpos_pad</th>\n      <th>...</th>\n      <th>is_far_left</th>\n      <th>is_far_right</th>\n      <th>score</th>\n      <th>pad_width</th>\n      <th>ball_speed</th>\n      <th>ball_down</th>\n      <th>xdist_ball_pad</th>\n      <th>ydist_ball_pad</th>\n      <th>l2_ball_pad</th>\n      <th>num_bricks_left</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>0</td>\n      <td>b'button1\\n'</td>\n      <td>False</td>\n      <td>118.267949</td>\n      <td>81.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>120.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>1.732051</td>\n      <td>62.0</td>\n      <td>62.024189</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>1</td>\n      <td>b'left\\n'</td>\n      <td>False</td>\n      <td>116.535898</td>\n      <td>82.0</td>\n      <td>118.267949</td>\n      <td>81.0</td>\n      <td>120.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>3.464102</td>\n      <td>61.0</td>\n      <td>61.098281</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>2</td>\n      <td>b'left\\n'</td>\n      <td>False</td>\n      <td>114.803848</td>\n      <td>83.0</td>\n      <td>116.535898</td>\n      <td>82.0</td>\n      <td>116.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>1.196152</td>\n      <td>60.0</td>\n      <td>60.011922</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>3</td>\n      <td>b'button1\\n'</td>\n      <td>False</td>\n      <td>113.071797</td>\n      <td>84.0</td>\n      <td>114.803848</td>\n      <td>83.0</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>1.071797</td>\n      <td>59.0</td>\n      <td>59.009734</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>4</td>\n      <td>b'button1\\n'</td>\n      <td>False</td>\n      <td>111.339746</td>\n      <td>85.0</td>\n      <td>113.071797</td>\n      <td>84.0</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>0.660254</td>\n      <td>58.0</td>\n      <td>58.003758</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>5</td>\n      <td>b'left\\n'</td>\n      <td>False</td>\n      <td>109.607695</td>\n      <td>86.0</td>\n      <td>111.339746</td>\n      <td>85.0</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>2.392305</td>\n      <td>57.0</td>\n      <td>57.050181</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>6</td>\n      <td>b'left\\n'</td>\n      <td>False</td>\n      <td>107.875644</td>\n      <td>87.0</td>\n      <td>109.607695</td>\n      <td>86.0</td>\n      <td>108.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>0.124356</td>\n      <td>56.0</td>\n      <td>56.000138</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>7</td>\n      <td>b'button1\\n'</td>\n      <td>False</td>\n      <td>106.143594</td>\n      <td>88.0</td>\n      <td>107.875644</td>\n      <td>87.0</td>\n      <td>104.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>2.143594</td>\n      <td>55.0</td>\n      <td>55.041757</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>8</td>\n      <td>b'button1\\n'</td>\n      <td>False</td>\n      <td>104.411543</td>\n      <td>89.0</td>\n      <td>106.143594</td>\n      <td>88.0</td>\n      <td>104.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>0.411543</td>\n      <td>54.0</td>\n      <td>54.001568</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>agent1</td>\n      <td>1004977</td>\n      <td>9</td>\n      <td>b'left\\n'</td>\n      <td>False</td>\n      <td>102.679492</td>\n      <td>90.0</td>\n      <td>104.411543</td>\n      <td>89.0</td>\n      <td>104.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>big</td>\n      <td>fast</td>\n      <td>True</td>\n      <td>1.320508</td>\n      <td>53.0</td>\n      <td>53.016448</td>\n      <td>108</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 24 columns</p>\n</div>","text/plain":"  agent_name     seed  t        action  missed_ball   xpos_ball  ypos_ball  \\\n0     agent1  1004977  0  b'button1\\n'        False  118.267949       81.0   \n1     agent1  1004977  1     b'left\\n'        False  116.535898       82.0   \n2     agent1  1004977  2     b'left\\n'        False  114.803848       83.0   \n3     agent1  1004977  3  b'button1\\n'        False  113.071797       84.0   \n4     agent1  1004977  4  b'button1\\n'        False  111.339746       85.0   \n5     agent1  1004977  5     b'left\\n'        False  109.607695       86.0   \n6     agent1  1004977  6     b'left\\n'        False  107.875644       87.0   \n7     agent1  1004977  7  b'button1\\n'        False  106.143594       88.0   \n8     agent1  1004977  8  b'button1\\n'        False  104.411543       89.0   \n9     agent1  1004977  9     b'left\\n'        False  102.679492       90.0   \n\n   xpos_ball_prev  ypos_ball_prev  xpos_pad  ...  is_far_left  is_far_right  \\\n0             NaN             NaN     120.0  ...        False         False   \n1      118.267949            81.0     120.0  ...        False         False   \n2      116.535898            82.0     116.0  ...        False         False   \n3      114.803848            83.0     112.0  ...        False         False   \n4      113.071797            84.0     112.0  ...        False         False   \n5      111.339746            85.0     112.0  ...        False         False   \n6      109.607695            86.0     108.0  ...        False         False   \n7      107.875644            87.0     104.0  ...        False         False   \n8      106.143594            88.0     104.0  ...        False         False   \n9      104.411543            89.0     104.0  ...        False         False   \n\n   score  pad_width  ball_speed  ball_down  xdist_ball_pad ydist_ball_pad  \\\n0      0        big        fast       True        1.732051           62.0   \n1      0        big        fast       True        3.464102           61.0   \n2      0        big        fast       True        1.196152           60.0   \n3      0        big        fast       True        1.071797           59.0   \n4      0        big        fast       True        0.660254           58.0   \n5      0        big        fast       True        2.392305           57.0   \n6      0        big        fast       True        0.124356           56.0   \n7      0        big        fast       True        2.143594           55.0   \n8      0        big        fast       True        0.411543           54.0   \n9      0        big        fast       True        1.320508           53.0   \n\n  l2_ball_pad  num_bricks_left  \n0   62.024189              108  \n1   61.098281              108  \n2   60.011922              108  \n3   59.009734              108  \n4   58.003758              108  \n5   57.050181              108  \n6   56.000138              108  \n7   55.041757              108  \n8   54.001568              108  \n9   53.016448              108  \n\n[10 rows x 24 columns]"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","pd.read_csv('dat.csv')\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}